<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-education/prompt-engineering/techniques" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">Techniques | Vibe Labs</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://EliFuzz.github.io/vibe-labs/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://EliFuzz.github.io/vibe-labs/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://EliFuzz.github.io/vibe-labs/docs/education/prompt-engineering/techniques"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Techniques | Vibe Labs"><meta data-rh="true" name="description" content="Prompt Techniques"><meta data-rh="true" property="og:description" content="Prompt Techniques"><link data-rh="true" rel="icon" href="/vibe-labs/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://EliFuzz.github.io/vibe-labs/docs/education/prompt-engineering/techniques"><link data-rh="true" rel="alternate" href="https://EliFuzz.github.io/vibe-labs/docs/education/prompt-engineering/techniques" hreflang="en"><link data-rh="true" rel="alternate" href="https://EliFuzz.github.io/vibe-labs/docs/education/prompt-engineering/techniques" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://H7KSU6B104-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Prompt Engineering","item":"https://EliFuzz.github.io/vibe-labs/docs/category/prompt-engineering"},{"@type":"ListItem","position":2,"name":"Techniques","item":"https://EliFuzz.github.io/vibe-labs/docs/education/prompt-engineering/techniques"}]}</script><link rel="search" type="application/opensearchdescription+xml" title="Vibe Labs" href="/vibe-labs/opensearch.xml">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/vibe-labs/assets/css/styles.75d7dbc5.css">
<script src="/vibe-labs/assets/js/runtime~main.1a0a2089.js" defer="defer"></script>
<script src="/vibe-labs/assets/js/main.9e765a69.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light";var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/vibe-labs/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_tM0j" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/vibe-labs/"><div class="navbar__logo"><img src="/vibe-labs/img/logo.svg" alt="Vibe Labs Logo" class="themedComponent_gZEv themedComponent--light_Bm9f"><img src="/vibe-labs/img/logo.svg" alt="Vibe Labs Logo" class="themedComponent_gZEv themedComponent--dark_yeZ7"></div><b class="navbar__title text--truncate">Vibe Labs</b></a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link" sidebarid="study">ðŸŽ“ Study</a><ul class="dropdown__menu"><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/vibe-labs/docs/category/prompt-engineering">ðŸ“– Education</a></li></ul></div></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/EliFuzz" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_zrOr"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle__EvU colorModeToggle_ghwn"><button class="clean-btn toggleButton_kgoQ toggleButtonDisabled_chwJ" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_pROH lightToggleIcon_vQf_"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_pROH darkToggleIcon_mAt_"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_pROH systemToggleIcon_cCxB"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_B0dD"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_nq3Q"><div class="docsWrapper_yJ3j"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_YxUW" type="button"></button><div class="docRoot_PQci"><aside class="theme-doc-sidebar-container docSidebarContainer_lZbW"><div class="sidebarViewport_o9ey"><div class="sidebar_TWDs"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_cd2Y"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/vibe-labs/docs/category/prompt-engineering">Prompt Engineering</a><button aria-label="Collapse sidebar category &#x27;Prompt Engineering&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/vibe-labs/docs/education/prompt-engineering/overview">Overview</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/vibe-labs/docs/education/prompt-engineering/guides">Guides</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/vibe-labs/docs/education/prompt-engineering/techniques">Techniques</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/vibe-labs/docs/education/prompt-engineering/risks">Risks</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/vibe-labs/docs/category/prompts">Prompts</a><button aria-label="Expand sidebar category &#x27;Prompts&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_wyab"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_QwOb"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_lRXH"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col"><div class="docItemContainer_m608"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_YiOx" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/vibe-labs/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_OUFm"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/vibe-labs/docs/category/prompt-engineering"><span>Prompt Engineering</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Techniques</span></li></ul></nav><div class="theme-doc-markdown markdown"><header><h1>Prompt Techniques</h1></header>
<table><thead><tr><th>Technique</th><th>Description</th><th>Mechanism</th><th>Benefits</th><th>Limitations</th><th>Use Cases</th></tr></thead><tbody><tr><td><b>Zero-shot Prompting</b></td><td><p>LLM directly prompted to perform a task without any prior specific
examples. It relies on the LLM&#x27;s pre-trained knowledge to infer an
appropriate response</p></td><td><p>The LLM leverages its extensive pre-trained knowledge base to understand
the given prompt and generate a relevant response without seeing any
task-specific examples. It&#x27;s about direct task instruction</p></td><td><p>Simplicity, works out-of-the-box for many tasks, ideal for quick queries</p></td><td><p>Can struggle with complex or novel tasks, may produce less accurate or
inconsistent results compared to other methods requiring examples</p></td><td><p>Question answering, summarization, simple text generation, language
translation</p></td></tr><tr><td><b>Few-shot Prompting</b></td><td><p>LLM is provided with a small number of input-output examples (typically
1-5) of a task within the prompt to guide its understanding and
generation of the desired output</p></td><td><p>The LLM observes the provided examples, learns the pattern and desired
behavior, and then applies this learned context to solve a new, unseen
input of the same task. It&#x27;s in-context learning</p></td><td><p>Significantly improves performance on various tasks, especially for
specific formats or styles; reduces the need for extensive fine-tuning;
adaptable to new tasks quickly</p></td><td><p>Performance is highly dependent on the quality and diversity of the
provided examples; prompt length can become an issue with too many
examples; still less robust than fine-tuning for highly specialized
tasks</p></td><td><p>Sentiment analysis, classification, question answering with specific
formats, code generation, creative writing</p></td></tr><tr><td><b>Chain-of-Thought (CoT) Prompting</b></td><td><p>Encourages LLMs to break down complex problems into a series of
intermediate reasoning steps before arriving at a final answer. This
mimics human-like thinking</p></td><td><p>The prompt explicitly instructs the LLM to &quot;think step by step&quot; or
provides examples of step-by-step reasoning. The LLM generates these
intermediate thoughts, which then guide its subsequent reasoning and
final answer generation</p></td><td><p>Improves accuracy on complex reasoning tasks (e.g., math word problems,
common sense reasoning); provides transparency into the model&#x27;s thought
process; enables the model to allocate more compute to problems
requiring more reasoning</p></td><td><p>Can be slower due to multiple reasoning steps; works best with larger
models; the generated reasoning might not always reflect the true
internal computation; can overcomplicate simple questions</p></td><td><p>Mathematical reasoning, symbolic manipulation, complex question
answering, common sense reasoning, programming</p></td></tr><tr><td><b>Meta Prompting</b></td><td><p>LLM is prompted to generate or refine its own prompts to better perform
a given task. It&#x27;s about using the LLM to optimize its own instructions</p></td><td><p>The user asks the LLM to create or modify a prompt for a specific goal.
Through iterative dialogue, the LLM refines the prompt based on user
feedback, aiming to produce a more effective instruction for itself or
another LLM</p></td><td><p>Improves prompt quality and effectiveness; reduces manual prompt
engineering effort; allows for dynamic prompt adaptation; can save time
in finding optimal prompts</p></td><td><p>Requires a good initial understanding of the desired outcome; can be
computationally intensive if many iterations are needed; the quality of
meta-prompts depends on the LLM&#x27;s ability to self-critique</p></td><td><p>Prompt optimization, creating tailored instructions for specific tasks,
refining conversational flows, content generation for specific
tones/styles</p></td></tr><tr><td><b>Self-Consistency</b></td><td><p>Enhances the reliability and accuracy of LLM outputs by generating
multiple diverse reasoning paths or responses to a given prompt and then
selecting the most consistent or frequently occurring answer among them</p></td><td><p>The LLM is prompted multiple times, often with CoT, to generate diverse
outputs. For quantitative answers, a majority vote selects the most
common one. For qualitative answers, the LLM itself or an external
evaluator determines the most coherent or consistent response</p></td><td><p>Improves accuracy, especially for tasks requiring reasoning or
interpretation (e.g., math, code generation); particularly useful for
free-form text generation where exact answers are not expected; reduces
hallucination</p></td><td><p>Can be computationally more expensive due to multiple generations; may
not always converge to a single &quot;best&quot; answer for highly subjective
tasks; requires a mechanism to evaluate consistency</p></td><td><p>Mathematical problem solving, code generation, summarization, open-ended
question answering, creative text generation</p></td></tr><tr><td><b>Generate Knowledge Prompting</b></td><td><p>LLM is prompted to first generate relevant knowledge or facts pertinent
to a query, and then use that generated knowledge to formulate its final
answer</p></td><td><p>The prompt is structured in two stages: first, an instruction to
generate background information, facts, or concepts related to the
query; second, an instruction to answer the original query using the
newly generated knowledge</p></td><td><p>Provides the LLM with a &quot;self-curated&quot; knowledge base for the specific
query, potentially leading to more accurate and grounded responses;
reduces reliance on only the model&#x27;s internal parameters for factual
accuracy; can mitigate hallucination</p></td><td><p>Adds an extra step to the reasoning process, increasing latency; quality
of the final answer depends on the accuracy and relevance of the
generated knowledge; can still generate incorrect knowledge</p></td><td><p>Factual question answering, research assistance, content creation
requiring specific factual grounding</p></td></tr><tr><td><b>Prompt Chaining</b></td><td><p>Involves breaking down a complex task into a series of smaller,
sequential prompts, where the output of one prompt serves as the input
or context for the next</p></td><td><p>A sequence of prompts is defined, each designed to perform a specific
sub-task. The LLM processes the first prompt, its output is captured,
and then concatenated with the next prompt in the chain, continuing
until the final desired output is achieved</p></td><td><p>Enables the handling of multi-step complex tasks; ensures coherence and
consistency across generated text; provides more control over the
generation process; can be used to build sophisticated conversational
agents</p></td><td><p>Requires careful design of each prompt in the chain; errors in earlier
prompts can propagate; debugging can be challenging; can be
resource-intensive for very long chains</p></td><td><p>Multi-turn conversations, data extraction pipelines, structured content
generation (e.g., reports, articles with specific sections), complex
problem-solving workflows</p></td></tr><tr><td><b>Tree of Thoughts (ToT)</b></td><td><p>An advanced reasoning framework that generalizes Chain-of-Thought by
exploring multiple reasoning paths in a tree-like structure, allowing
for backtracking and exploration of alternative solutions</p></td><td><p>Instead of a single linear chain, the LLM generates multiple &quot;thoughts&quot;
(intermediate reasoning steps) at each stage. These thoughts form
branches, and the system can evaluate their viability, prune unpromising
paths, and backtrack to explore others, often using a search algorithm
(e.g., Breadth-First Search, Depth-First Search)</p></td><td><p>Significantly improves performance on tasks requiring deeper, strategic
thinking and decision-making; allows for more robust exploration of
problem spaces; can mitigate early errors by exploring alternative paths</p></td><td><p>Computationally intensive due to the exploration of multiple paths;
complex to implement and fine-tune; requires careful design of
evaluation functions for pruning and path selection</p></td><td><p>Planning, complex problem-solving (e.g., game playing, logistics),
creative generation with constraints, code debugging</p></td></tr><tr><td><b>Retrieval Augmented Generation (RAG)</b></td><td><p>A framework that combines the generative capabilities of LLMs with
external information retrieval systems (e.g., databases, search engines)
to ground the LLM&#x27;s responses in factual, up-to-date knowledge</p></td><td><p>When a query is received, a retrieval component searches an external
knowledge base for relevant documents or information. This retrieved
information is then provided as context to the LLM, which uses both its
internal knowledge and the external data to generate a more accurate and
informed response</p></td><td><p>Addresses LLM limitations like hallucination and outdated information;
provides access to fresh, domain-specific, or proprietary data; enhances
factual accuracy and trustworthiness of outputs; reduces the need for
constant model retraining</p></td><td><p>Requires an efficient and relevant retrieval system; can be complex to
set up and maintain the knowledge base; potential for noise if retrieved
information is irrelevant or low quality; latency introduced by the
retrieval step</p></td><td><p>Factual question answering, enterprise chatbots, knowledge management,
summarization of specific documents, legal research</p></td></tr><tr><td><b>Automatic Reasoning and Tool-use (ART)</b></td><td><p>A framework that enables LLMs to automatically break down problems,
reason through steps, and dynamically use external tools (e.g.,
calculators, search engines, code interpreters) to solve complex tasks</p></td><td><p>ART maintains a task library of example problems and their step-by-step
solutions, and a tool library. When facing a new problem, it finds
similar examples, guides the LLM to generate a step-by-step solution,
and automatically pauses for tool usage, coordinating between the LLM&#x27;s
thought process and external tools</p></td><td><p>Enhances LLM capabilities beyond pure language generation; improves
accuracy on multi-step reasoning problems; reduces reliance on manual
prompt engineering for tool integration; increases problem-solving
flexibility</p></td><td><p>Can suffer from cascading errors if early steps are incorrect;
performance is limited by the quality of generated code or tool usage;
task selection from the library isn&#x27;t always perfect; integration with
various tools can be complex</p></td><td><p>Scientific problem solving, complex data analysis, interactive agents,
technical support, educational tutoring systems</p></td></tr><tr><td><b>Automatic Prompt Engineer (APE)</b></td><td><p>An innovative solution where an AI system autonomously generates,
optimizes, and selects prompts to achieve desired LLM outputs,
significantly reducing manual effort</p></td><td><p>APE typically receives input-output pairs as examples. It then uses
techniques like reinforcement learning, gradient-based optimization, or
meta-prompting to generate and evaluate candidate prompts. The process
iterates, refining prompts based on feedback on how well the generated
responses match the expected outputs</p></td><td><p>Dramatically reduces the time and effort required for prompt
engineering; can discover highly effective prompts that humans might
miss; improves consistency and quality of LLM outputs across various
applications</p></td><td><p>Can be computationally expensive during the optimization process;
requires significant labeled data for training the APE system;
interpretability of automatically generated prompts can be low</p></td><td><p>Optimizing prompts for chatbots, content generation, data extraction,
fine-tuning LLM behavior for specific tasks</p></td></tr><tr><td><b>Active-Prompt</b></td><td><p>Improves Chain-of-Thought (CoT) prompting performance by selectively
human-annotating exemplars (examples) where the model shows the most
uncertainty</p></td><td><p>Instead of uniformly sampling examples, Active-Prompt identifies
instances where the LLM is most uncertain about its reasoning path.
These &quot;uncertain&quot; instances are then prioritized for human annotation to
create high-quality CoT exemplars, which are then used to improve the
LLM&#x27;s performance</p></td><td><p>Focuses human annotation effort on the most impactful examples;
potentially achieves better performance with less human labeling work;
improves the robustness of CoT reasoning</p></td><td><p>Requires a mechanism to quantify LLM uncertainty; involves
human-in-the-loop for selective annotation; can be more complex to
implement than standard CoT</p></td><td><p>Improving CoT for specific domains or challenging tasks where the model
frequently errs; active learning scenarios for prompt engineering</p></td></tr><tr><td><b>Directional Stimulus Prompting (DSP)</b></td><td><p>A framework for guiding black-box LLMs towards specific desired outputs
by introducing subtle, instance-specific &quot;directional stimulus&quot; (hints
or cues) generated by a smaller, tunable policy model</p></td><td><p>A small policy model (e.g., a fine-tuned T5) generates an auxiliary
stimulus prompt for each input. This stimulus acts as a hint, subtly
guiding the larger, black-box LLM (e.g., GPT-3, GPT-4) towards a desired
outcome, without directly adjusting the LLM&#x27;s parameters. The policy
model is optimized via supervised fine-tuning and reinforcement learning</p></td><td><p>Provides fine-grained, query-specific guidance for black-box LLMs;
bypasses the need for direct LLM fine-tuning; allows for more controlled
output generation (e.g., including specific keywords); can enhance
reasoning accuracy</p></td><td><p>Relies on the effectiveness of the smaller policy model; can still guide
the LLM towards biased or harmful content if the policy model is
optimized incorrectly; adds an extra layer of complexity</p></td><td><p>Summarization with specific keyword requirements, dialogue response
generation, controlled text generation, guiding LLMs for specific
stylistic outputs</p></td></tr><tr><td><b>Program-Aided Language Models (PAL)</b></td><td><p>A novel approach that combines LLMs with external interpreters (e.g.,
Python) to improve their reasoning capabilities, particularly for
mathematical, logical, and algorithmic problems</p></td><td><p>The LLM interprets the natural language prompt and, during its reasoning
step, generates a program (e.g., Python code) that encapsulates the
logic required to solve the problem. This generated program is then
executed by an external interpreter, and the result is used to formulate
the final answer</p></td><td><p>Significantly improves accuracy on tasks requiring precise computation
or symbolic manipulation; offloads complex calculations to reliable
external tools; allows LLMs to leverage programming logic beyond their
internal neural network capabilities</p></td><td><p>Requires the LLM to have sufficient coding ability; introduces a
dependency on external interpreters; debugging errors in generated code
can be challenging; execution time includes code generation and
interpretation</p></td><td><p>Mathematical problem solving, data processing, logical puzzles,
algorithmic tasks, generating and executing SQL queries</p></td></tr><tr><td><b>ReAct</b></td><td><p>A prompting strategy that interweaves <code>Thought</code> (reasoning)
and <code>Action</code> (tool-use) steps, allowing LLMs to dynamically
reason and interact with external environments to gather information or
perform operations</p></td><td><p>The LLM generates a <code>Thought</code> which explains its reasoning
process, then an <code>Action</code> which specifies a tool to use
(e.g., search engine, calculator) and its arguments. The tool&#x27;s output
is observed, and the LLM then generates another <code>Thought</code> and<!-- --> <!-- -->
<code>Action</code> based on the new information, iterating until the
task is complete</p></td><td><p>Enables LLMs to perform multi-step tasks requiring external knowledge or
computation; provides a transparent trace of reasoning and actions;
allows for dynamic planning and adaptation to environmental feedback;
reduces hallucinations by grounding responses in real-world data</p></td><td><p>Can be complex to design effective action spaces and tool integrations;
performance heavily relies on the quality of tools and the LLM&#x27;s ability
to interpret tool outputs; potential for infinite loops or incorrect
action sequences</p></td><td><p>Complex question answering requiring external data, interactive problem
solving, web Browse, task automation, scientific experimentation</p></td></tr><tr><td><b>Reflexion</b></td><td><p>A general prompting strategy that involves having LLMs analyze their own
outputs, behaviors, knowledge, or reasoning processes to identify errors
and iteratively improve their performance</p></td><td><p>The LLM generates an initial output. A &quot;reflection&quot; module (which can be
another LLM or a set of rules) then critically assesses this output,
providing feedback, critique, or recommendations for improvement. The
original LLM then uses this feedback to generate a refined output,
repeating the process until a satisfactory result is achieved</p></td><td><p>Enables self-correction and continuous improvement without human
intervention; enhances robustness and reliability of LLM outputs;
provides a mechanism for learning from past mistakes; useful for tasks
requiring high accuracy</p></td><td><p>Can be computationally intensive due to iterative refinement; the
quality of reflection heavily depends on the reflection mechanism;
potential for &quot;reflection loops&quot; if the model struggles to self-correct;
may not always converge to the optimal solution</p></td><td><p>Code debugging and improvement, content refinement, factual correction,
complex task completion where iterative improvement is beneficial</p></td></tr><tr><td><b>Multimodal CoT</b></td><td><p>Extends the Chain-of-Thought reasoning to incorporate multiple
modalities, typically language (text) and vision (images), allowing LLMs
to reason over information presented in different forms</p></td><td><p>In a two-stage framework, the LLM first generates intermediate reasoning
chains (rationales) based on information from both text and image
inputs. Then, in the second stage, it uses these multimodal rationales
to infer the final answer</p></td><td><p>Enables LLMs to solve problems that require understanding and
integrating information from diverse modalities; improves performance on
multimodal reasoning tasks; can mitigate hallucination by grounding
reasoning in visual evidence</p></td><td><p>Requires multimodal LLMs capable of processing and integrating different
data types; complexity in aligning information across modalities; the
quality of rationales depends on the integration of multimodal inputs</p></td><td><p>Visual question answering (VQA), science questions with diagrams, image
captioning with reasoning, medical diagnosis from reports and scans</p></td></tr><tr><td><b>Graph Prompting</b></td><td><p>Prompts LLMs by treating knowledge or information as a structured graph
(e.g., knowledge graphs), allowing the LLM to leverage relational
information</p></td><td><p>The knowledge is preprocessed and presented to the LLM as a structured
graph or a description derived from it, highlighting entities and their
relationships. The prompt guides the LLM to reason over this graph
structure to answer queries or generate text</p></td><td><p>Leverages the rich relational information present in knowledge graphs;
improves factual consistency and reasoning over structured data; can
enhance understanding of complex relationships between entities; allows
for more precise and grounded responses</p></td><td><p>Requires the existence or construction of a knowledge graph; converting
information into a graph structure can be complex; the LLM needs to be
adept at interpreting graph representations</p></td><td><p>Question answering over knowledge graphs, fact extraction, entity
linking, semantic search, reasoning in complex domains (e.g.,
biomedical, legal)</p></td></tr></tbody></table></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/EliFuzz/vibe-labs/docs/education/01-prompt-engineering/03-techniques.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_vTzu" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_tLxJ"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/vibe-labs/docs/education/prompt-engineering/guides"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Guides</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/vibe-labs/docs/education/prompt-engineering/risks"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Risks</div></a></nav></div></div></div></div></main></div></div></div></div>
</body>
</html>