"use strict";(self.webpackChunkclassic=self.webpackChunkclassic||[]).push([[2930],{7601:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>m,frontMatter:()=>a,metadata:()=>i,toc:()=>u});const i=JSON.parse('{"id":"education/prompts/PRD/success-metrics","title":"Success Metrics","description":"Success metrics of the PRD","source":"@site/docs/education/02-prompts/01-PRD/12-success-metrics.mdx","sourceDirName":"education/02-prompts/01-PRD","slug":"/education/prompts/PRD/success-metrics","permalink":"/vibe-labs/docs/education/prompts/PRD/success-metrics","draft":false,"unlisted":false,"editUrl":"https://github.com/EliFuzz/vibe-labs/docs/education/02-prompts/01-PRD/12-success-metrics.mdx","tags":[],"version":"current","sidebarPosition":12,"frontMatter":{"title":"Success Metrics","description":"Success metrics of the PRD","hide_table_of_contents":true},"sidebar":"education","previous":{"title":"Product Overview","permalink":"/vibe-labs/docs/education/prompts/PRD/product-overview"},"next":{"title":"Product Requirements","permalink":"/vibe-labs/docs/education/prompts/PRD/product-requirements"}}');var s=t(3420),r=t(8906);const a={title:"Success Metrics",description:"Success metrics of the PRD",hide_table_of_contents:!0},o=void 0,c={},u=[];function d(e){const n={code:"code",pre:"pre",...(0,r.R)(),...e.components};return(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-markdown",children:"# Success Metrics & KPIs\n\n## Purpose\n\nDefine comprehensive measurement framework that tracks product success across business, user, and technical dimensions. This section establishes how success will be measured, monitored, and optimized.\n\n## Prerequisites\n\n- Business objectives clearly defined\n- User personas and success criteria established\n- Functional and technical requirements completed\n- SRE framework and SLI/SLO requirements established\n- Performance engineering requirements defined\n- UX requirements and usability goals defined\n\n## Section Structure & Requirements\n\n### 1. Measurement Framework\n\n**Objective**: Establish overall approach to measuring success\n\n**Required Elements:**\n\n- **Measurement Philosophy**: Approach to metrics and measurement\n- **Metric Categories**: How metrics are organized and categorized\n- **Measurement Cadence**: How often metrics are reviewed and reported\n- **Accountability Framework**: Who is responsible for each metric\n- **Data Collection Strategy**: How metrics data will be collected and validated\n\n**Quality Criteria:**\n\n- Framework is comprehensive yet focused\n- Metrics align with business objectives and user needs\n- Measurement approach is sustainable and actionable\n- Accountability is clear and appropriate\n\n**Template:**\n\n## Measurement Framework\n\n### Measurement Philosophy\n\n[Approach to metrics and measurement]\n\n### Metric Categories\n\n- **Business Metrics**: [Revenue, growth, market share, etc.]\n- **User Metrics**: [Engagement, satisfaction, retention, etc.]\n- **Product Metrics**: [Usage, adoption, performance, etc.]\n- **SRE Metrics**: [SLI/SLO compliance, error budgets, reliability, etc.]\n- **Performance Metrics**: [Latency, throughput, capacity, efficiency, etc.]\n- **Technical Metrics**: [Performance, reliability, security, etc.]\n\n### Measurement Cadence\n\n- **Daily Metrics**: [Metrics reviewed daily]\n- **Weekly Metrics**: [Metrics reviewed weekly]\n- **Monthly Metrics**: [Metrics reviewed monthly]\n- **Quarterly Metrics**: [Metrics reviewed quarterly]\n\n### Accountability Framework\n\n[Who is responsible for each metric category]\n\n### Data Collection Strategy\n\n[How metrics data will be collected and validated]\n\n### 2. Business Success Metrics\n\n**Objective**: Define metrics that measure business value and impact\n\n**Required Elements:**\n\n- **Revenue Metrics**: Direct and indirect revenue impact\n- **Growth Metrics**: User acquisition, market expansion, product adoption\n- **Efficiency Metrics**: Cost reduction, process improvement, productivity gains\n- **Market Metrics**: Market share, competitive positioning, brand impact\n- **ROI Metrics**: Return on investment and payback period\n\n**Quality Criteria:**\n\n- Metrics directly tie to business objectives\n- Targets are ambitious yet achievable\n- Metrics can be influenced by product decisions\n- Baseline and benchmark data is available or obtainable\n\n**Template:**\n\n## Business Success Metrics\n\n### Revenue Metrics\n\n- **[Metric Name]**: [Definition, target, measurement method]\n- **[Metric Name]**: [Definition, target, measurement method]\n\n### Growth Metrics\n\n- **[Metric Name]**: [Definition, target, measurement method]\n- **[Metric Name]**: [Definition, target, measurement method]\n\n### Efficiency Metrics\n\n- **[Metric Name]**: [Definition, target, measurement method]\n- **[Metric Name]**: [Definition, target, measurement method]\n\n### Market Metrics\n\n- **[Metric Name]**: [Definition, target, measurement method]\n- **[Metric Name]**: [Definition, target, measurement method]\n\n### ROI Metrics\n\n- **[Metric Name]**: [Definition, target, measurement method]\n- **[Metric Name]**: [Definition, target, measurement method]\n\n### 2.1. SRE & Reliability Metrics\n\n**Objective**: Define metrics that measure system reliability and SRE effectiveness\n\n**Required Elements:**\n\n- **Service Level Indicators (SLIs)**: Metrics that measure service performance\n- **Service Level Objectives (SLOs)**: Target values for SLIs\n- **Error Budget Metrics**: How error budgets are tracked and consumed\n- **Toil Metrics**: Measurement of operational toil and automation progress\n- **Incident Metrics**: Metrics related to incident frequency and resolution\n\n**Template:**\n\n## SRE & Reliability Metrics\n\n### Service Level Indicators (SLIs)\n\n- **Availability SLI**: [% of successful requests over time window]\n- **Latency SLI**: [95th percentile response time]\n- **Throughput SLI**: [Requests per second capacity]\n- **Quality SLI**: [% of requests returning correct results]\n\n### Service Level Objectives (SLOs)\n\n- **Availability SLO**: [99.9% availability over 30-day window]\n- **Latency SLO**: [95th percentile < 200ms over 30-day window]\n- **Throughput SLO**: [Handle 10,000 RPS sustained load]\n\n### Error Budget Metrics\n\n- **Error Budget Remaining**: [% of error budget remaining]\n- **Error Budget Burn Rate**: [Rate of error budget consumption]\n- **Error Budget Alerts**: [Alerts when error budget is at risk]\n\n### Toil Metrics\n\n- **Toil Percentage**: [% of time spent on toil vs. engineering work]\n- **Automation Coverage**: [% of operational tasks automated]\n- **Manual Intervention Rate**: [Frequency of manual interventions]\n\n### Incident Metrics\n\n- **Mean Time to Detection (MTTD)**: [Time to detect incidents]\n- **Mean Time to Resolution (MTTR)**: [Time to resolve incidents]\n- **Incident Frequency**: [Number of incidents per time period]\n\n### 3. User Success Metrics\n\n**Objective**: Define metrics that measure user value and satisfaction\n\n**Required Elements:**\n\n- **Adoption Metrics**: How users discover and start using the product\n- **Engagement Metrics**: How actively users engage with the product\n- **Retention Metrics**: How well the product retains users over time\n- **Satisfaction Metrics**: How satisfied users are with the product\n- **Value Realization Metrics**: How well users achieve their goals\n\n**Quality Criteria:**\n\n- Metrics reflect real user value and satisfaction\n- Metrics can be tracked across user segments\n- Targets are based on user research and benchmarks\n- Metrics predict long-term user success\n\n**Template:**\n\n## User Success Metrics\n\n### Adoption Metrics\n\n- **User Acquisition Rate**: [New users per period]\n- **Activation Rate**: [% of users who complete key onboarding actions]\n- **Time to First Value**: [Time until user achieves first success]\n\n### Engagement Metrics\n\n- **Daily/Weekly/Monthly Active Users**: [Active user counts]\n- **Session Duration**: [Average time spent per session]\n- **Feature Adoption**: [% of users using key features]\n- **User Actions per Session**: [Average actions per session]\n\n### Retention Metrics\n\n- **User Retention Rate**: [% of users returning after X days/weeks/months]\n- **Churn Rate**: [% of users who stop using product]\n- **Cohort Retention**: [Retention by user cohort]\n\n### Satisfaction Metrics\n\n- **Net Promoter Score (NPS)**: [User recommendation likelihood]\n- **Customer Satisfaction (CSAT)**: [User satisfaction ratings]\n- **User Effort Score (UES)**: [Ease of use ratings]\n\n### Value Realization Metrics\n\n- **Goal Completion Rate**: [% of users achieving their goals]\n- **Task Success Rate**: [% of tasks completed successfully]\n- **Time to Goal Achievement**: [Time to achieve user goals]\n\n### 4. Product Performance Metrics\n\n**Objective**: Define metrics that measure product quality and performance\n\n**Required Elements:**\n\n- **Usage Metrics**: How the product is being used\n- **Performance Metrics**: Technical performance and reliability\n- **Quality Metrics**: Bugs, errors, and quality issues\n- **Feature Metrics**: Individual feature performance and adoption\n- **Content Metrics**: Content effectiveness and engagement\n\n**Template:**\n\n## Product Performance Metrics\n\n### Usage Metrics\n\n- **Feature Usage**: [Usage rates for key features]\n- **User Flows**: [Completion rates for key user flows]\n- **Content Consumption**: [Content views, downloads, shares]\n\n### Performance Metrics\n\n- **Page Load Time**: [Average page load times]\n- **API Response Time**: [Average API response times]\n- **Uptime**: [System availability percentage]\n- **Error Rate**: [Error rate for key operations]\n\n### Quality Metrics\n\n- **Bug Report Rate**: [Bugs reported per user/session]\n- **Crash Rate**: [Application crash frequency]\n- **Support Ticket Volume**: [Support requests per user]\n\n### Feature Metrics\n\n- **Feature Adoption Rate**: [% of users adopting new features]\n- **Feature Retention**: [Continued usage of features over time]\n- **Feature Satisfaction**: [User satisfaction with specific features]\n\n### 5. Leading and Lagging Indicators\n\n**Objective**: Balance predictive and outcome metrics\n\n**Required Elements:**\n\n- **Leading Indicators**: Metrics that predict future success\n- **Lagging Indicators**: Metrics that measure achieved outcomes\n- **Correlation Analysis**: How leading indicators relate to outcomes\n- **Early Warning Signals**: Metrics that indicate potential problems\n- **Predictive Models**: How metrics are used to predict future performance\n\n### 6. Measurement Implementation\n\n**Objective**: Define how metrics will be implemented and managed\n\n**Required Elements:**\n\n- **Data Collection Requirements**: What data needs to be collected\n- **Analytics Implementation**: Tools and systems for measurement\n- **Reporting Framework**: How metrics are reported and visualized\n- **Alert Systems**: When and how stakeholders are notified of issues\n- **Metric Evolution**: How metrics will evolve as product matures\n\n**Template:**\n\n## Measurement Implementation\n\n### Data Collection Requirements\n\n[What data needs to be collected and how]\n\n### Analytics Implementation\n\n- **Analytics Tools**: [Google Analytics, Mixpanel, custom analytics, etc.]\n- **Data Warehouse**: [Where data is stored and processed]\n- **Real-time vs Batch**: [Which metrics are real-time vs batch processed]\n\n### Reporting Framework\n\n- **Dashboards**: [Key dashboards and their audiences]\n- **Reports**: [Regular reports and their frequency]\n- **Visualization**: [How metrics are visualized and presented]\n\n### Alert Systems\n\n[When and how stakeholders are notified of issues]\n\n### Metric Evolution\n\n[How metrics will evolve as product matures]\n\n## Information Gathering Requirements\n\n### Metrics Context Needed:\n\n- Business objectives and success criteria\n- User research insights and success definitions\n- Current baseline metrics and benchmarks\n- Competitive metrics and industry standards\n- Technical capabilities for data collection\n\n### Validation Requirements:\n\n- Stakeholder alignment on metric definitions and targets\n- Technical validation of measurement feasibility\n- User research validation of success definitions\n- Benchmark analysis for target setting\n\n## Cross-Reference Requirements\n\n### Must Reference:\n\n- Business objectives and success criteria\n- User personas and their success definitions\n- Functional requirements and feature priorities\n- Technical capabilities and constraints\n\n### Must Support:\n\n- Product optimization and iteration decisions\n- Business reporting and stakeholder communication\n- User research and validation activities\n- Technical monitoring and alerting\n\n## Common Pitfalls to Avoid\n\n### Metric Selection Pitfalls:\n\n- **Vanity metrics**: Choosing metrics that look good but don't drive decisions\n- **Too many metrics**: Tracking everything instead of focusing on what matters\n- **Lagging indicators only**: Not having predictive metrics\n- **Metric gaming**: Creating metrics that can be easily manipulated\n\n### Target Setting Pitfalls:\n\n- **Unrealistic targets**: Setting impossible goals that demotivate teams\n- **No baseline**: Setting targets without understanding current performance\n- **Static targets**: Not adjusting targets as you learn and grow\n- **Conflicting targets**: Setting targets that work against each other\n\n### Implementation Pitfalls:\n\n- **Data quality issues**: Not ensuring data accuracy and reliability\n- **Measurement overhead**: Making measurement too complex or expensive\n- **Delayed implementation**: Not implementing measurement from launch\n- **No action framework**: Collecting metrics but not acting on insights\n\n## Edge Case Considerations\n\n### When Baseline Data is Unavailable:\n\n- Use industry benchmarks and competitive analysis\n- Plan rapid baseline establishment after launch\n- Use proxy metrics until direct measurement is possible\n- Document assumptions and validation plans\n\n### When User Privacy Limits Measurement:\n\n- Focus on aggregate and anonymized metrics\n- Use privacy-preserving measurement techniques\n- Balance measurement needs with privacy requirements\n- Plan for evolving privacy regulations\n\n### When Technical Constraints Limit Measurement:\n\n- Prioritize most critical metrics for initial implementation\n- Plan measurement infrastructure improvements\n- Use sampling and estimation techniques\n- Document measurement limitations and their impact\n\n## Validation Checkpoints\n\n### Before Finalizing Section:\n\n- [ ] Metrics align with business objectives and user needs\n- [ ] Targets are realistic and based on research/benchmarks\n- [ ] Measurement approach is technically feasible\n- [ ] Leading and lagging indicators are balanced\n- [ ] Implementation plan is detailed and actionable\n\n### Cross-Section Validation:\n\n- [ ] Metrics support product vision and strategy\n- [ ] Success definitions align with user research\n- [ ] Measurement requirements fit technical architecture\n- [ ] Reporting framework supports decision-making needs\n\n## Output Quality Standards\n\n- Metrics are specific, measurable, and actionable\n- Targets are ambitious yet achievable\n- Implementation plan is detailed and feasible\n- Framework balances different stakeholder needs\n- Content enables effective measurement and optimization\n"})})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8906:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var i=t(6672);const s={},r=i.createContext(s);function a(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);